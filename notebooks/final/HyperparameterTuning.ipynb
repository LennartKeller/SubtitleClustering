{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import *\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../dataset/movies_complete.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "\n",
    "We use different features:\n",
    "1. Tfidf-Features (different params)\n",
    "    1. on raw text\n",
    "    2. on lemmas\n",
    "2. Fasttext-Embedding (mean, max, min pooled)\n",
    "3. Bert-Embeddings (mean, max, min pooled)\n",
    "4. DistilBert-Embeddings (Head and Tail)\n",
    "5. All other embeddings concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def read_embeddings(embedding_file: str):\n",
    "    data = Path(embedding_file).read_text()\n",
    "    lines = data.split('\\n')\n",
    "    filenames = []\n",
    "    embeddings = []\n",
    "    for line in lines:\n",
    "        line_data = line.split(' ')\n",
    "        if len(line_data) >= 2:\n",
    "            filenames.append(line_data[0])\n",
    "            embeddings.append(list(map(float, line_data[1:])))\n",
    "    return np.asarray(filenames), np.asarray(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_filenames, embeddings_mean = read_embeddings('../../dataset/embeddings_mean.txt')\n",
    "max_filenames, embeddings_max = read_embeddings('../../dataset/embeddings_max.txt')\n",
    "min_filenames, embeddings_min = read_embeddings('../../dataset/embeddings_min.txt')\n",
    "\n",
    "bert_mean_filenames, bert_mean = read_embeddings('../../dataset/embeddings_bert_mean.txt')\n",
    "bert_max_filenames, bert_max = read_embeddings('../../dataset/embeddings_bert_max.txt')\n",
    "bert_min_filenames, bert_min = read_embeddings('../../dataset/embeddings_bert_min.txt')\n",
    "\n",
    "bert_headtail_filenames, bert_headtail = read_embeddings('../../dataset/embeddings_bert_headtail.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_headtail[bert_headtail.shape == (0,)] = np.zeros((768,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embeddings = []\n",
    "max_embeddings = []\n",
    "min_embeddings = []\n",
    "\n",
    "mean_bert = []\n",
    "max_bert = []\n",
    "min_bert = []\n",
    "ht_bert = []\n",
    "for index, row in df.iterrows():\n",
    "    mean_embeddings.append(embeddings_mean[np.where(mean_filenames == row.filename)].ravel())\n",
    "    max_embeddings.append(embeddings_max[np.where(max_filenames == row.filename)].ravel())\n",
    "    min_embeddings.append(embeddings_min[np.where(min_filenames == row.filename)].ravel())\n",
    "    \n",
    "    mean_bert.append((bert_mean[np.where(bert_mean_filenames == row.filename)].ravel()))\n",
    "    max_bert.append((bert_max[np.where(bert_max_filenames == row.filename)].ravel()))\n",
    "    min_bert.append((bert_min[np.where(bert_min_filenames == row.filename)].ravel()))\n",
    "    \n",
    "    ht_bert.append((bert_headtail[np.where(bert_headtail_filenames == row.filename)].ravel()))\n",
    "    \n",
    "df['fasttext_mean'] = mean_embeddings\n",
    "df['fasttext_max'] = max_embeddings\n",
    "df['fasttext_min'] = min_embeddings\n",
    "\n",
    "df['bert_mean'] = mean_bert\n",
    "df['bert_max'] = max_bert\n",
    "df['bert_min'] = min_bert\n",
    "\n",
    "mean_embeddings = np.asarray(mean_embeddings)\n",
    "max_embeddings = np.asarray(max_embeddings)\n",
    "min_embeddings = np.asarray(min_embeddings)\n",
    "\n",
    "mean_bert = np.asarray(mean_bert)\n",
    "max_bert = np.asarray(max_bert)\n",
    "min_bert = np.asarray(min_bert)\n",
    "\n",
    "ht_bert = np.asarray(ht_bert)\n",
    "\n",
    "\n",
    "del embeddings_mean, embeddings_max, embeddings_min, bert_mean, bert_max, bert_min, bert_headtail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_comb = np.hstack([\n",
    "    mean_embeddings,\n",
    "    max_embeddings,\n",
    "    min_embeddings,\n",
    "    mean_bert,\n",
    "    max_bert,\n",
    "    min_bert,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': [500, 1000, 2500, 5000, 10000, 15000, 20000],\n",
       " 'max_df': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "        0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n",
       " 'stop_words': [['aber',\n",
       "   'alle',\n",
       "   'allem',\n",
       "   'allen',\n",
       "   'aller',\n",
       "   'alles',\n",
       "   'als',\n",
       "   'also',\n",
       "   'am',\n",
       "   'an',\n",
       "   'ander',\n",
       "   'andere',\n",
       "   'anderem',\n",
       "   'anderen',\n",
       "   'anderer',\n",
       "   'anderes',\n",
       "   'anderm',\n",
       "   'andern',\n",
       "   'anders',\n",
       "   'auch',\n",
       "   'auf',\n",
       "   'aus',\n",
       "   'bei',\n",
       "   'bin',\n",
       "   'bis',\n",
       "   'bist',\n",
       "   'da',\n",
       "   'damit',\n",
       "   'dann',\n",
       "   'das',\n",
       "   'dass',\n",
       "   'dasselbe',\n",
       "   'dazu',\n",
       "   'daß',\n",
       "   'dein',\n",
       "   'deine',\n",
       "   'deinem',\n",
       "   'deinen',\n",
       "   'deiner',\n",
       "   'deines',\n",
       "   'dem',\n",
       "   'demselben',\n",
       "   'den',\n",
       "   'denn',\n",
       "   'denselben',\n",
       "   'der',\n",
       "   'derer',\n",
       "   'derselbe',\n",
       "   'derselben',\n",
       "   'des',\n",
       "   'desselben',\n",
       "   'dessen',\n",
       "   'dich',\n",
       "   'die',\n",
       "   'dies',\n",
       "   'diese',\n",
       "   'dieselbe',\n",
       "   'dieselben',\n",
       "   'diesem',\n",
       "   'diesen',\n",
       "   'dieser',\n",
       "   'dieses',\n",
       "   'dir',\n",
       "   'doch',\n",
       "   'dort',\n",
       "   'du',\n",
       "   'durch',\n",
       "   'ein',\n",
       "   'eine',\n",
       "   'einem',\n",
       "   'einen',\n",
       "   'einer',\n",
       "   'eines',\n",
       "   'einig',\n",
       "   'einige',\n",
       "   'einigem',\n",
       "   'einigen',\n",
       "   'einiger',\n",
       "   'einiges',\n",
       "   'einmal',\n",
       "   'er',\n",
       "   'es',\n",
       "   'etwas',\n",
       "   'euch',\n",
       "   'euer',\n",
       "   'eure',\n",
       "   'eurem',\n",
       "   'euren',\n",
       "   'eurer',\n",
       "   'eures',\n",
       "   'für',\n",
       "   'gegen',\n",
       "   'gewesen',\n",
       "   'hab',\n",
       "   'habe',\n",
       "   'haben',\n",
       "   'hat',\n",
       "   'hatte',\n",
       "   'hatten',\n",
       "   'hier',\n",
       "   'hin',\n",
       "   'hinter',\n",
       "   'ich',\n",
       "   'ihm',\n",
       "   'ihn',\n",
       "   'ihnen',\n",
       "   'ihr',\n",
       "   'ihre',\n",
       "   'ihrem',\n",
       "   'ihren',\n",
       "   'ihrer',\n",
       "   'ihres',\n",
       "   'im',\n",
       "   'in',\n",
       "   'indem',\n",
       "   'ins',\n",
       "   'ist',\n",
       "   'jede',\n",
       "   'jedem',\n",
       "   'jeden',\n",
       "   'jeder',\n",
       "   'jedes',\n",
       "   'jene',\n",
       "   'jenem',\n",
       "   'jenen',\n",
       "   'jener',\n",
       "   'jenes',\n",
       "   'jetzt',\n",
       "   'kann',\n",
       "   'kein',\n",
       "   'keine',\n",
       "   'keinem',\n",
       "   'keinen',\n",
       "   'keiner',\n",
       "   'keines',\n",
       "   'können',\n",
       "   'könnte',\n",
       "   'machen',\n",
       "   'man',\n",
       "   'manche',\n",
       "   'manchem',\n",
       "   'manchen',\n",
       "   'mancher',\n",
       "   'manches',\n",
       "   'mein',\n",
       "   'meine',\n",
       "   'meinem',\n",
       "   'meinen',\n",
       "   'meiner',\n",
       "   'meines',\n",
       "   'mich',\n",
       "   'mir',\n",
       "   'mit',\n",
       "   'muss',\n",
       "   'musste',\n",
       "   'nach',\n",
       "   'nicht',\n",
       "   'nichts',\n",
       "   'noch',\n",
       "   'nun',\n",
       "   'nur',\n",
       "   'ob',\n",
       "   'oder',\n",
       "   'ohne',\n",
       "   'sehr',\n",
       "   'sein',\n",
       "   'seine',\n",
       "   'seinem',\n",
       "   'seinen',\n",
       "   'seiner',\n",
       "   'seines',\n",
       "   'selbst',\n",
       "   'sich',\n",
       "   'sie',\n",
       "   'sind',\n",
       "   'so',\n",
       "   'solche',\n",
       "   'solchem',\n",
       "   'solchen',\n",
       "   'solcher',\n",
       "   'solches',\n",
       "   'soll',\n",
       "   'sollte',\n",
       "   'sondern',\n",
       "   'sonst',\n",
       "   'um',\n",
       "   'und',\n",
       "   'uns',\n",
       "   'unser',\n",
       "   'unsere',\n",
       "   'unserem',\n",
       "   'unseren',\n",
       "   'unserer',\n",
       "   'unseres',\n",
       "   'unter',\n",
       "   'viel',\n",
       "   'vom',\n",
       "   'von',\n",
       "   'vor',\n",
       "   'war',\n",
       "   'waren',\n",
       "   'warst',\n",
       "   'was',\n",
       "   'weg',\n",
       "   'weil',\n",
       "   'weiter',\n",
       "   'welche',\n",
       "   'welchem',\n",
       "   'welchen',\n",
       "   'welcher',\n",
       "   'welches',\n",
       "   'wenn',\n",
       "   'werde',\n",
       "   'werden',\n",
       "   'wie',\n",
       "   'wieder',\n",
       "   'will',\n",
       "   'wir',\n",
       "   'wird',\n",
       "   'wirst',\n",
       "   'wo',\n",
       "   'wollen',\n",
       "   'wollte',\n",
       "   'während',\n",
       "   'würde',\n",
       "   'würden',\n",
       "   'zu',\n",
       "   'zum',\n",
       "   'zur',\n",
       "   'zwar',\n",
       "   'zwischen',\n",
       "   'über'],\n",
       "  None],\n",
       " 'ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "from itertools import repeat\n",
    "\n",
    "tfidf_params = {\n",
    "    'max_features': [500, 1000, 2500, 5000, 10000, 15000, 20000],\n",
    "    'max_df': np.linspace(0, 1, 10),\n",
    "    'stop_words': [get_stop_words('de'), None],\n",
    "    'ngram_range': [(i, j) for i, j in zip(repeat(1), range(1, 6))]\n",
    "}\n",
    "\n",
    "tfidf_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define custom evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_size_std(labels):\n",
    "    cluster, cluster_sizes = np.unique(labels, return_counts=True)\n",
    "    return cluster_sizes.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define tuning function\n",
    "\n",
    "We do not use sklearns GridsearchCV class since we do not want to use cross-validation.\n",
    "Rather we use metrics which are computed on the clusters found while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import ClusterMixin\n",
    "from tqdm import tqdm\n",
    "from sklearn.base import clone as clone_estimator\n",
    "from time import time\n",
    "\n",
    "def _call_scoring(entry, score, data):\n",
    "    try:\n",
    "        entry[score.__name__] = score(data)\n",
    "    except Exception as e:\n",
    "        entry['Exception'] = e\n",
    "    return entry\n",
    "\n",
    "def tune(clu: ClusterMixin,\n",
    "         X: np.ndarray,\n",
    "         params: ParameterGrid,\n",
    "         scoring: callable = None,\n",
    "         verbose: bool = True):\n",
    "    \n",
    "    results = []\n",
    "    params = tqdm(params) if verbose else params\n",
    "    for param_comb in params:\n",
    "        entry = {**param_comb}\n",
    "        clu_instance = clone_estimator(clu)\n",
    "        \n",
    "        fit_start = time()\n",
    "        clu_instance.fit(X)\n",
    "        fit_time = time() - fit_start\n",
    "        entry['fit_time'] = fit_time\n",
    "        \n",
    "        labels = clu_instance.predict(X)\n",
    "        \n",
    "        if scoring:\n",
    "            if isinstance(scoring, list):\n",
    "                for score in scoring:\n",
    "                    _call_scoring(entry=entry, score=score, data=labels)\n",
    "            if callable(scoring):\n",
    "                _call_scoring(entry=entry, score=scoring, data=labels)\n",
    "            else:\n",
    "                raise Exception('Invalid scoring parameter passed')\n",
    "        else:\n",
    "            if hasattr(clu_instance, 'score') and callable(clu_instance.score):\n",
    "                _call_scoring(entry=entry, score=clu_instance.score, data=X)\n",
    "        results.append(entry)\n",
    "        \n",
    "    return pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_params_dicts(*args, sep='__'):\n",
    "    merged = {}\n",
    "    for entry in args:\n",
    "        if isinstance(entry, tuple):\n",
    "            prefix = entry[0]\n",
    "            merged.update({prefix + sep + key: value for key, value in entry[1].items()})\n",
    "        if isinstance(entry, dict):\n",
    "            merged.update(entry)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pca__n_components': 2, 'kmeans__n_cluster': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_params_dicts(('pca', {'n_components': 2}), ('kmeans', {'n_cluster': 2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clustering-Algorithm: KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_params = {\n",
    "    'max_iter': [300, 500, 1000],\n",
    "    'n_cluster': list(range(1, 21, 3)),  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()), ('kmeans', KMeans())])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "kmeans = KMeans()\n",
    "# monkey patch kmeans score method to get inertia\n",
    "def get_intertia(self):\n",
    "    return self.inertia_\n",
    "kmeans.score = get_intertia\n",
    "\n",
    "kmeans_tdidf = make_pipeline(TfidfVectorizer(), kmeans)\n",
    "kmeans_tdidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean_tfidf_params = ParameterGrid(\n",
    "    merge_params_dicts(('tfidfvectorizer', tfidf_params), ('kmeans', kmeans_params))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:17<00:00, 77.34s/it]\n"
     ]
    }
   ],
   "source": [
    "kmean_tfidf_rawtext = tune(clu=kmeans_tdidf,\n",
    "                           X=df.text,\n",
    "                           params=kmean_tfidf_params,\n",
    "                           verbose=True)\n",
    "kmean_tfidf_rawtext.to_csv('../../Results/KMeans_Tfidf_Rawtext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean_tfidf_lemma = tune(clu=kmeans_tdidf,\n",
    "                           X=df.lemma,\n",
    "                           params=kmean_tfidf_params,\n",
    "                           verbose=True)\n",
    "kmean_tfidf_lemma.to_csv('../../Results/KMeans_Tfidf_Lemma.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
