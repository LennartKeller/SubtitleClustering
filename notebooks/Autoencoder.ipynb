{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal:\n",
    "\n",
    "We want to build a character-based recurrent autoencoder (using attention and lstm recurrent units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/movies_complete.csv')\n",
    "df.text = df.text.str.replace('\\s+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters) +1 # for unknown token\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df.text.apply(unicodeToAscii).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def textToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts_encoded = [textToTensor(text) for text in texts[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(texts)\n",
    "\n",
    "char_dict = {char: index + 1 for index, char in enumerate(all_letters)}\n",
    "char_dict[tk.oov_token] = max(char_dict.values()) + 1\n",
    "tk.word_index = char_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tk.texts_to_sequences(texts)\n",
    "X = tk.texts_to_matrix(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 20000\n",
    "\n",
    "sequences = pad_sequences(sequences=sequences, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1080.9324034334763"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(maxlen - np.count_nonzero(sequences, axis=1)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n",
    "\n",
    "1. Attempt in keras\n",
    "2. If that works port to pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # use id from $ nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_inputs = Input(shape=(maxlen,), name='Encoder-Input')\n",
    "#emb_layer = Embedding(n_letters, 150,input_length = maxlen, name='Body-Word-Embedding', mask_zero=False)\n",
    "#\n",
    "#x = emb_layer(encoder_inputs)\n",
    "#state_h = Bidirectional(LSTM(128, activation='relu', name='Encoder-Last-LSTM'))(x)\n",
    "#encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "#seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "#\n",
    "#decoded = RepeatVector(maxlen)(seq2seq_encoder_out)\n",
    "#decoder_lstm = Bidirectional(LSTM(128, return_sequences=True, name='Decoder-LSTM-before'))\n",
    "#decoder_lstm_output = decoder_lstm(decoded)\n",
    "#decoder_dense = Dense(n_letters, activation='softmax', name='Final-Output-Dense-before')\n",
    "#decoder_outputs = decoder_dense(decoder_lstm_output)\n",
    "#\n",
    "#seq2seq_Model = Model(encoder_inputs, decoder_outputs)\n",
    "#seq2seq_Model.compile(Nadam(lr=0.001), loss='sparse_categorical_crossentropy')\n",
    "#history = seq2seq_Model.fit(sequences, sequences,\n",
    "#          batch_size=16,\n",
    "#          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_23 (Sequential)   (None, 30)                173112    \n",
      "_________________________________________________________________\n",
      "sequential_24 (Sequential)   (None, 20000, 57)         58157     \n",
      "=================================================================\n",
      "Total params: 231,269\n",
      "Trainable params: 231,269\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "recurrent_encoder = Sequential([\n",
    "\n",
    "Embedding(input_dim=n_letters, output_dim=256, input_length=maxlen),\n",
    "LSTM(100, return_sequences=True),\n",
    "LSTM(30)\n",
    "])\n",
    "recurrent_decoder = Sequential([\n",
    "RepeatVector(maxlen, input_shape=[30]),\n",
    "LSTM(100, return_sequences=True),\n",
    "TimeDistributed(Dense(n_letters, activation=\"sigmoid\"))\n",
    "])\n",
    "\n",
    "recurrent_ae = Sequential([recurrent_encoder, recurrent_decoder])\n",
    "recurrent_ae.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "recurrent_ae.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
