{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.textpath import TextPath\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn import mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vereinfachte Methode:\n",
    "def linkage_matrix(n_samples, children, distances):\n",
    "    \"\"\"\n",
    "    create a linkage matrix for the dendogram method in scipy\n",
    "    n_samples: int, number of samples\n",
    "    children: list of lists, clustered data points (should be 2)\n",
    "    distances: list of distances between nodes\n",
    "    \"\"\"\n",
    "    # Create linkage matrix\n",
    "    \n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(children.shape[0])\n",
    "    for i, merge in enumerate(children):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    return np.column_stack([children, distances, counts]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3728, 56)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../dataset/movies_complete.csv')\n",
    "#dfs ist ein Verweis auf den ganzen Dataframe, weil df im folgenden gesliced wird\n",
    "dfs = df.sample(frac=1)\n",
    "dfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3728, 56)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.dropna(subset=['text', 'genre', 'year', 'production_region'], inplace=True)\n",
    "dfs.drop_duplicates(subset=['IMDB_ID'], inplace=True)\n",
    "dfs.shape\n",
    "df.drop_duplicates(subset=['IMDB_ID'], inplace=True)\n",
    "dfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>IMDB_ID</th>\n",
       "      <th>genre</th>\n",
       "      <th>year</th>\n",
       "      <th>production_region</th>\n",
       "      <th>corpus</th>\n",
       "      <th>duration</th>\n",
       "      <th>directors</th>\n",
       "      <th>...</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Metascore</th>\n",
       "      <th>imdbRating</th>\n",
       "      <th>imdbVotes</th>\n",
       "      <th>Type</th>\n",
       "      <th>DVD</th>\n",
       "      <th>BoxOffice</th>\n",
       "      <th>Production</th>\n",
       "      <th>Website</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1396</td>\n",
       "      <td>1396</td>\n",
       "      <td>4494465.xml</td>\n",
       "      <td>IM DUNKELN sind alle Wölfe grau Pfui Deibel , ...</td>\n",
       "      <td>tt2102465</td>\n",
       "      <td>Action,Crime,Thriller</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Norwegian</td>\n",
       "      <td>untokenisiert</td>\n",
       "      <td>87.0</td>\n",
       "      <td>nm1103121</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'Source': 'Internet Movie Database', 'Value'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1,146</td>\n",
       "      <td>movie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3210</td>\n",
       "      <td>3210</td>\n",
       "      <td>6696047.xml</td>\n",
       "      <td>Es war das Jahr , an das sich alle erinnerten ...</td>\n",
       "      <td>tt3838728</td>\n",
       "      <td>Drama</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>French</td>\n",
       "      <td>untokenisiert</td>\n",
       "      <td>98.0</td>\n",
       "      <td>nm0404067</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'Source': 'Internet Movie Database', 'Value'...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2,416</td>\n",
       "      <td>movie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Full House Films</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1888</td>\n",
       "      <td>1888</td>\n",
       "      <td>4562212.xml</td>\n",
       "      <td>2012 Nach 2 Jahren der \" Rettung \" haben die \"...</td>\n",
       "      <td>tt2385027</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Greek</td>\n",
       "      <td>untokenisiert</td>\n",
       "      <td>87.0</td>\n",
       "      <td>nm4414980,nm4415168</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'Source': 'Internet Movie Database', 'Value'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1</td>\n",
       "      <td>435</td>\n",
       "      <td>movie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3532</td>\n",
       "      <td>3532</td>\n",
       "      <td>6729676.xml</td>\n",
       "      <td>Ok . Ah , das ist es . - Ok . - Ok . - Na also...</td>\n",
       "      <td>tt4438848</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>untokenisiert</td>\n",
       "      <td>92.0</td>\n",
       "      <td>nm0831557</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'Source': 'Internet Movie Database', 'Value'...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>109,853</td>\n",
       "      <td>movie</td>\n",
       "      <td>20 Sep 2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Universal Pictures</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3498</td>\n",
       "      <td>3498</td>\n",
       "      <td>6717187.xml</td>\n",
       "      <td>- Scheiße , ich werde dich vermissen . - Ja . ...</td>\n",
       "      <td>tt4074364</td>\n",
       "      <td>Drama</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>untokenisiert</td>\n",
       "      <td>105.0</td>\n",
       "      <td>nm1323584</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'Source': 'Internet Movie Database', 'Value'...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>3,418</td>\n",
       "      <td>movie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Film Väst</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     filename  \\\n",
       "1396        1396  4494465.xml   \n",
       "3210        3210  6696047.xml   \n",
       "1888        1888  4562212.xml   \n",
       "3532        3532  6729676.xml   \n",
       "3498        3498  6717187.xml   \n",
       "\n",
       "                                                   text    IMDB_ID  \\\n",
       "1396  IM DUNKELN sind alle Wölfe grau Pfui Deibel , ...  tt2102465   \n",
       "3210  Es war das Jahr , an das sich alle erinnerten ...  tt3838728   \n",
       "1888  2012 Nach 2 Jahren der \" Rettung \" haben die \"...  tt2385027   \n",
       "3532  Ok . Ah , das ist es . - Ok . - Ok . - Na also...  tt4438848   \n",
       "3498  - Scheiße , ich werde dich vermissen . - Ja . ...  tt4074364   \n",
       "\n",
       "                      genre    year production_region         corpus  \\\n",
       "1396  Action,Crime,Thriller  2011.0         Norwegian  untokenisiert   \n",
       "3210                  Drama  2015.0            French  untokenisiert   \n",
       "1888            Documentary  2012.0             Greek  untokenisiert   \n",
       "3532                 Comedy  2016.0               NaN  untokenisiert   \n",
       "3498                  Drama  2016.0               NaN  untokenisiert   \n",
       "\n",
       "      duration            directors  ...  \\\n",
       "1396      87.0            nm1103121  ...   \n",
       "3210      98.0            nm0404067  ...   \n",
       "1888      87.0  nm4414980,nm4415168  ...   \n",
       "3532      92.0            nm0831557  ...   \n",
       "3498     105.0            nm1323584  ...   \n",
       "\n",
       "                                                Ratings Metascore imdbRating  \\\n",
       "1396  [{'Source': 'Internet Movie Database', 'Value'...       NaN        6.6   \n",
       "3210  [{'Source': 'Internet Movie Database', 'Value'...      53.0        5.5   \n",
       "1888  [{'Source': 'Internet Movie Database', 'Value'...       NaN        8.1   \n",
       "3532  [{'Source': 'Internet Movie Database', 'Value'...      58.0        5.7   \n",
       "3498  [{'Source': 'Internet Movie Database', 'Value'...      58.0        5.8   \n",
       "\n",
       "     imdbVotes   Type          DVD  BoxOffice          Production  Website  \\\n",
       "1396     1,146  movie          NaN        NaN                 NaN      NaN   \n",
       "3210     2,416  movie          NaN        NaN    Full House Films      NaN   \n",
       "1888       435  movie          NaN        NaN                 NaN      NaN   \n",
       "3532   109,853  movie  20 Sep 2016        NaN  Universal Pictures      NaN   \n",
       "3498     3,418  movie          NaN        NaN           Film Väst      NaN   \n",
       "\n",
       "      Response  \n",
       "1396      True  \n",
       "3210      True  \n",
       "1888      True  \n",
       "3532      True  \n",
       "3498      True  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering über Zeit mit Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "def read_embeddings(embedding_file: str):\n",
    "    data = Path(embedding_file).read_text()\n",
    "    lines = data.split('\\n')\n",
    "    filenames = []\n",
    "    embeddings = []\n",
    "    for line in lines:\n",
    "        line_data = line.split(' ')\n",
    "        if len(line_data) >=2 :\n",
    "            filenames.append(line_data[0])\n",
    "            embeddings.append(list(map(float, line_data[1:])))\n",
    "    return np.asarray(filenames), np.asarray(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\dataset\\\\embeddings.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6655805bc729>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfilenames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../dataset/embeddings.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-671ede5d264d>\u001b[0m in \u001b[0;36mread_embeddings\u001b[1;34m(embedding_file)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_file\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mfilenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\pathlib.py\u001b[0m in \u001b[0;36mread_text\u001b[1;34m(self, encoding, errors)\u001b[0m\n\u001b[0;32m   1204\u001b[0m         \u001b[0mOpen\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mclose\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m         \"\"\"\n\u001b[1;32m-> 1206\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1191\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         return io.open(self, mode, buffering, encoding, errors, newline,\n\u001b[1;32m-> 1193\u001b[1;33m                        opener=self._opener)\n\u001b[0m\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\pathlib.py\u001b[0m in \u001b[0;36m_opener\u001b[1;34m(self, name, flags, mode)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0o666\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m         \u001b[1;31m# A stub for the opener argument to built-in open()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_raw_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0o777\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\dataset\\\\embeddings.txt'"
     ]
    }
   ],
   "source": [
    "filenames, embeddings = read_embeddings('../dataset/embeddings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames.shape, embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['filename'].isin(filenames)].copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.filename == filenames).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Einfügen der Embeddings ins Dataframe\n",
    "df['embedding'] = [e for e in embeddings]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot über Zeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df['year']).plot.bar()\n",
    "print('Mean: ', df['year'].mean(), 'Median: ', df['year'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split der Daten in Zeitperioden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sincevalue = 2017\n",
    "#beforevalue = 2000\n",
    "#since = df.year >= sincevalue\n",
    "#before = df.year <= beforevalue\n",
    "#years = since + before\n",
    "#years.shape\n",
    "colorlist = []\n",
    "\n",
    "for x in df.year:\n",
    "    if x >=2017:\n",
    "        colorlist.append('red')\n",
    "    elif x <= 2007:\n",
    "        colorlist.append('blue')\n",
    "    else: \n",
    "        colorlist.append('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labelcolor']=colorlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2)\n",
    "model.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(embeddings)\n",
    "X_tf = pca.transform(embeddings)\n",
    "\n",
    "#Abspeichern der Zwischenergebnisste im Datafram\n",
    "df['pca2d_1'] = X_tf[:,0]\n",
    "df['pca2d_2'] = X_tf[:,1]\n",
    "df['kmeans'] = model.labels_\n",
    "\n",
    "#Plotten anhand der gespeicherten Zwischenergebnisse \n",
    "plt.figure(figsize=(15,8))\n",
    "for d in df.iterrows():\n",
    "    x = d[1].pca2d_1\n",
    "    y = d[1].pca2d_2    \n",
    "    plt.plot(x, y, 'o', markersize=4, color= d[1].labelcolor)\n",
    "    plt.plot(x, y, marker=TextPath((-3, -3), f'{d[1].kmeans}'), color='black', markersize=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(embeddings)\n",
    "X_tf = pca.transform(embeddings)\n",
    "print(len(X_tf))\n",
    "df['pca2d_1'] = X_tf[:,0]\n",
    "df['pca2d_2'] = X_tf[:,1]\n",
    "df['kmeans'] = model.labels_\n",
    "\n",
    "df_clean = df[df.labelcolor != 'white']\n",
    "#df_embedding = df_clean['embedding']\n",
    "#df_embedding.tolist()\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "for d in df_clean.iterrows():\n",
    "    x = d[1].pca2d_1\n",
    "    y = d[1].pca2d_2    \n",
    "    plt.plot(x, y, 'o', markersize=8, color= d[1].labelcolor)\n",
    "    plt.plot(x, y, marker=TextPath((-3, -3), f'{d[1].kmeans}'), color='white', markersize=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verteilung der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = df.loc[(df['kmeans'] == 1) & (df['labelcolor'] == 'blue')]\n",
    "b0 = df.loc[(df['kmeans'] == 0) & (df['labelcolor'] == 'blue')]\n",
    "r1 = df.loc[(df['kmeans'] == 1) & (df['labelcolor'] == 'red')]\n",
    "r0 = df.loc[(df['kmeans'] == 0) & (df['labelcolor'] == 'red')]\n",
    "print('b1: ', len(b1), 'b0: ', len(b0), 'r1: ', len(r1), 'r0: ', len(r0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ohne Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zwei Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.dropna(subset=['text', 'year', 'genre', 'duration', 'runtimeMinutes'], inplace=True)\n",
    "plt.figure(figsize=(15,8))\n",
    "pd.value_counts(dfs['year']).plot.bar()\n",
    "dfs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einschub Titletype - Was ist in Filme enthalten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie VS. TVMovie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"ein Netflix Original\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(dfs['titleType']).plot.bar()\n",
    "pd.value_counts(dfs['titleType'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_movie = dfs.loc[dfs['titleType'] == 'movie']\n",
    "dfs_movie.shape\n",
    "plt.figure(figsize=(15,8))\n",
    "pd.value_counts(dfs_movie['genre']).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "genre_combination_frequs = Counter(dfs_movie.genre)\n",
    "genre_combination_frequs.most_common(20)\n",
    "#len(genre_combination_frequs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(dfs_movie['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "pd.value_counts(dfs_movie['year']).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_tvmovie = dfs.loc[dfs['titleType'] == 'tvMovie']\n",
    "dfs_tvmovie\n",
    "plt.figure(figsize=(15,8))\n",
    "pd.value_counts(dfs_tvmovie['genre']).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "genre_combination_frequs = Counter(dfs_tvmovie.genre)\n",
    "genre_combination_frequs.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_series_long = dfs.loc[(dfs['titleType'] == 'tvEpisode') & (dfs['runtimeMinutes'].astype('int') >= 50)]\n",
    "dfs_series_long\n",
    "plt.figure(figsize=(15,8))\n",
    "pd.value_counts(dfs_series_long['genre']).plot.bar()\n",
    "dfs_series_long.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(dfs_series_long['year']).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing 2 Cluster Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Aufteilung in 1950 bis 1970 und 2016\n",
    "colors = []\n",
    "for x in dfs.year:\n",
    "    if x == 2016:\n",
    "        colors.append('red')\n",
    "    elif (x <= 1970) & (x >= 1940):\n",
    "        colors.append('blue')\n",
    "    else: \n",
    "        colors.append('white')\n",
    "dfs['colors']=colors\n",
    "      \n",
    "dfs_clean = dfs[dfs.colors != 'white']\n",
    "pd.value_counts(dfs_clean.colors).plot.bar() \n",
    "pd.value_counts(dfs_clean.colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means mit TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "tokens = tfidf.fit_transform(dfs_clean.text)\n",
    "#vectorizer = CountVectorizer()\n",
    "#X = vectorizer.fit_transform(df_split.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.toarray(), tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = KMeans(n_clusters=2)\n",
    "models.fit(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(models.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(tokens.todense())\n",
    "X_tf = pca.transform(tokens.todense())\n",
    "\n",
    "#Abspeichern der Zwischenergebnisste im Datafram\n",
    "dfs_clean['pcatokens_1'] = X_tf[:,0]\n",
    "dfs_clean['pcatokens_2'] = X_tf[:,1]\n",
    "dfs_clean['kmeanstokens'] = models.labels_\n",
    "\n",
    "#Plotten anhand der gespeicherten Zwischenergebnisse \n",
    "plt.figure(figsize=(15,8))\n",
    "for d in dfs_clean.iterrows():\n",
    "    x = d[1].pcatokens_1\n",
    "    y = d[1].pcatokens_2    \n",
    "    plt.plot(x, y, 'o', markersize=4, color= d[1].colors)\n",
    "    plt.plot(x, y, marker=TextPath((-3, -3), f'{d[1].kmeanstokens}'), color='black', markersize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs_cleans = dfs_clean.sample(frac=0.1)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "tokens2 = tfidf.fit_transform(dfs_cleans.text)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(tokens2.todense())\n",
    "X_tf = pca.transform(tokens2.todense())\n",
    "\n",
    "#Abspeichern der Zwischenergebnisste im Datafram\n",
    "dfs_cleans['pcatokens_1'] = X_tf[:,0]\n",
    "dfs_cleans['pcatokens_2'] = X_tf[:,1]\n",
    "\n",
    "#Plotten anhand der gespeicherten Zwischenergebnisse \n",
    "plt.figure(figsize=(15,8))\n",
    "for d in dfs_cleans.iterrows():\n",
    "    x = d[1].pcatokens_1\n",
    "    y = d[1].pcatokens_2    \n",
    "    plt.plot(x, y, 'o', markersize=8, color= d[1].colors)\n",
    "    plt.plot(x, y, marker=TextPath((-3, -3), f'{d[1].kmeanstokens}'), color='white', markersize=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergebnisverteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = dfs_cleans.loc[(dfs_cleans['kmeanstokens'] == 1) & (dfs_cleans['colors'] == 'blue')]\n",
    "b0 = dfs_cleans.loc[(dfs_cleans['kmeanstokens'] == 0) & (dfs_cleans['colors'] == 'blue')]\n",
    "r1 = dfs_cleans.loc[(dfs_cleans['kmeanstokens'] == 1) & (dfs_cleans['colors'] == 'red')]\n",
    "r0 = dfs_cleans.loc[(dfs_cleans['kmeanstokens'] == 0) & (dfs_cleans['colors'] == 'red')]\n",
    "print('b1: ', len(b1), 'b0: ', len(b0), 'r1: ', len(r1), 'r0: ', len(r0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing 3 Cluster Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aufteilung in bis 1960, 1980 bis 1985 und 2017\n",
    "colors = []\n",
    "for x in dfs.year:\n",
    "    if x <= 1960:\n",
    "        colors.append('red')\n",
    "    elif (x <= 1985) & (x >= 1980):\n",
    "        colors.append('blue')\n",
    "    elif x == 2017:\n",
    "        colors.append('green')\n",
    "    else: \n",
    "        colors.append('white')\n",
    "dfs['colors']=colors\n",
    "      \n",
    "dfs_clean3 = dfs[dfs.colors != 'white']\n",
    "pd.value_counts(dfs_clean3.colors).plot.bar() \n",
    "pd.value_counts(dfs_clean3.colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "tokens3 = tfidf.fit_transform(dfs_clean3.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models3 = KMeans(n_clusters=3)\n",
    "models3.fit(tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(tokens3.todense())\n",
    "X_tf = pca.transform(tokens3.todense())\n",
    "\n",
    "#Abspeichern der Zwischenergebnisste im Datafram\n",
    "dfs_clean3['pcatokens3_1'] = X_tf[:,0]\n",
    "dfs_clean3['pcatokens3_2'] = X_tf[:,1]\n",
    "dfs_clean3['kmeanstokens3'] = models3.labels_\n",
    "\n",
    "#Plotten anhand der gespeicherten Zwischenergebnisse \n",
    "plt.figure(figsize=(15,8))\n",
    "for d in dfs_clean3.iterrows():\n",
    "    x = d[1].pcatokens3_1\n",
    "    y = d[1].pcatokens3_2    \n",
    "    plt.plot(x, y, 'o', markersize=4, color= d[1].colors)\n",
    "    plt.plot(x, y, marker=TextPath((-3, -3), f'{d[1].kmeanstokens3}'), color='black', markersize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_cleansample3 = dfs_clean3.sample(frac=0.1)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "tokens3c = tfidf.fit_transform(dfs_cleansample3.text)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(tokens3c.todense())\n",
    "X_tf = pca.transform(tokens3c.todense())\n",
    "\n",
    "#Abspeichern der Zwischenergebnisste im Datafram\n",
    "dfs_cleansample3['pcatokens3_1'] = X_tf[:,0]\n",
    "dfs_cleansample3['pcatokens3_2'] = X_tf[:,1]\n",
    "\n",
    "#Plotten anhand der gespeicherten Zwischenergebnisse \n",
    "plt.figure(figsize=(15,8))\n",
    "for d in dfs_cleansample3.iterrows():\n",
    "    x = d[1].pcatokens3_1\n",
    "    y = d[1].pcatokens3_2    \n",
    "    plt.plot(x, y, 'o', markersize=8, color= d[1].colors)\n",
    "    plt.plot(x, y, marker=TextPath((-3, -3), f'{d[1].kmeanstokens3}'), color='white', markersize=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = dfs_cleansample3.loc[(dfs_cleansample3['kmeanstokens3'] == 1) & (dfs_cleansample3['colors'] == 'blue')]\n",
    "b0 = dfs_cleansample3.loc[(dfs_cleansample3['kmeanstokens3'] == 0) & (dfs_cleansample3['colors'] == 'blue')]\n",
    "b2 = dfs_cleansample3.loc[(dfs_cleansample3['kmeanstokens3'] == 2) & (dfs_cleansample3['colors'] == 'blue')]\n",
    "r1 = dfs_cleansample3.loc[(dfs_cleansample3['kmeanstokens3'] == 1) & (dfs_cleansample3['colors'] == 'red')]\n",
    "r0 = dfs_cleansample3.loc[(dfs_cleansample3['kmeanstokens3'] == 0) & (dfs_cleansample3['colors'] == 'red')]\n",
    "r2 = dfs_cleansample3.loc[(dfs_cleansample3['kmeanstokens3'] == 2) & (dfs_cleansample3['colors'] == 'red')]\n",
    "g0 = dfs_cleansample3.loc[(dfs_cleansample3['kmeanstokens3'] == 0) & (dfs_cleansample3['colors'] == 'green')]\n",
    "g1 = dfs_cleansample3.loc[(dfs_cleansample3['kmeanstokens3'] == 1) & (dfs_cleansample3['colors'] == 'green')]\n",
    "g2 = dfs_cleansample3.loc[(dfs_cleansample3['kmeanstokens3'] == 2) & (dfs_cleansample3['colors'] == 'green')]\n",
    "print('b1: ', len(b1), 'b0: ', len(b0), 'r1: ', len(r1), 'r0: ', len(r0))\n",
    "print('    0 - 1 - 2 \\ng: ', len(g0), len(g1), len(g2), '\\nb: ', len(b0), len(b1), len(b2), '\\nr: ', len(r0), len(r1), len(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchisches Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1940-1970 und 2016\n",
    "dfs_clean.shape\n",
    "# Datensatz samplen\n",
    "dfs_cleans = dfs_clean.sample(frac=0.008)\n",
    "pd.value_counts(dfs_cleans.colors).plot.bar() \n",
    "pd.value_counts(dfs_cleans.colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "dfs_cleans_tfids = tfidf.fit_transform(dfs_cleans.text)\n",
    "\n",
    "hier_model = AgglomerativeClustering(n_clusters=None, distance_threshold=0).fit(dfs_cleans_tfids.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmatrix = linkage_matrix(dfs_cleans_tfids.shape[0], hier_model.children_, hier_model.distances_)\n",
    "plt.figure(figsize=(15,8))\n",
    "dendrogram(lmatrix, labels=dfs_cleans['year'].values, leaf_font_size=10); #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hellblau: 35, davon 23 mit Label 2016 und 12 mit Label zwischen 1940-1970 ---\n",
    "Rot: 25, davon 5 mit Label 2016 und 20 mit Label zwischen 1940-1970 ---\n",
    "Grün: 4, 2 mal 2016 und 1 mit Label zwischen 1940-1970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster mit zwei Variablen Zeit und Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.dropna(subset=['text', 'year', 'genre', 'duration', 'runtimeMinutes'], inplace=True)\n",
    "plt.figure(figsize=(15,8))\n",
    "pd.value_counts(dfs['year']).plot.bar()\n",
    "dfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "pd.value_counts(dfs['genre']).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drama und horror bei 2017 und 2007\n",
    "labelcolor2var = []\n",
    "for index, row in dfs.iterrows():\n",
    "    if (row['genre'] == 'Drama') & (row['year'] == 2017) == True:\n",
    "        labelcolor2var.append('red')    #red\n",
    "    elif (row['genre'] == 'Drama') & (row['year'] == 2007) == True:\n",
    "        labelcolor2var.append('white') #orange\n",
    "    elif (row['genre'] == 'Comedy') & (row['year'] == 2017) == True:\n",
    "        labelcolor2var.append('blue')   #blue\n",
    "    elif (row['genre'] == 'Comedy') & (row['year'] == 2007) == True:\n",
    "        labelcolor2var.append('white')  #green\n",
    "    else: \n",
    "        labelcolor2var.append('white')\n",
    "dfs['labelcolor2var']=labelcolor2var    \n",
    "        \n",
    "dfs_2var = dfs[dfs.labelcolor2var != 'white']\n",
    "pd.value_counts(dfs_2var.labelcolor2var).plot.bar() \n",
    "pd.value_counts(dfs_2var.labelcolor2var)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans 2 Variablen Zeit und Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "tokens_2var = tfidf.fit_transform(dfs_2var.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMeans\n",
    "model_2var = KMeans(n_clusters=4)\n",
    "model_2var.fit(tokens_2var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(tokens_2var.todense())\n",
    "X_tf = pca.transform(tokens_2var.todense())\n",
    "\n",
    "#Abspeichern der Zwischenergebnisste im Datafram\n",
    "dfs_2var['pca2var1'] = X_tf[:,0]\n",
    "dfs_2var['pca2var2'] = X_tf[:,1]\n",
    "dfs_2var['kmeans2var'] = model_2var.labels_\n",
    "\n",
    "#Plotten anhand der gespeicherten Zwischenergebnisse \n",
    "plt.figure(figsize=(15,8))\n",
    "for d in dfs_2var.iterrows():\n",
    "    x = d[1].pca2var1\n",
    "    y = d[1].pca2var2    \n",
    "    plt.plot(x, y, 'o', markersize=4, color= d[1].labelcolor2var)\n",
    "    plt.plot(x, y, marker=TextPath((-3, -3), f'{d[1].kmeans2var}'), color='black', markersize=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hierarchisches Clustering mit 2 Variablen Zeit und Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "tokens_2var = tfidf.fit_transform(dfs_2var.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_model = AgglomerativeClustering(n_clusters=None, distance_threshold=0).fit(tokens_2var.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmatrix = linkage_matrix(tokens_2var.shape[0], hier_model.children_, hier_model.distances_)\n",
    "plt.figure(figsize=(15,8))\n",
    "dendrogram(lmatrix, labels=(dfs_2var['originalTitle'].values + '    ' + dfs_2var['labelcolor2var'].values), leaf_font_size=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blue ist Comedy, 2017 und rot ist Drama, 2017 ; orange ist Drama 2007 ; green ist Comedy 2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ohne Serien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs.dropna(subset=['text', 'year', 'genre', 'duration', 'runtimeMinutes'], inplace=True)\n",
    "dfs_movie = dfs.loc[dfs['titleType'] == 'movie']\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "from collections import Counter\n",
    "genre_combination_frequs = Counter(dfs_movie.genre)\n",
    "genre_combination_frequs.most_common(50)\n",
    "\n",
    "#pd.value_counts(dfs_movie['genre']).plot.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "pd.value_counts(dfs_movie['year']).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfs_movie' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d2afabe05b6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Drama und horror bei 2017 und 2007\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlabelcolor2var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdfs_movie\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'genre'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Documentary'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mlabelcolor2var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'red'\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m#red\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dfs_movie' is not defined"
     ]
    }
   ],
   "source": [
    "#Drama und horror bei 2017 und 2007\n",
    "labelcolor2var = []\n",
    "for index, row in dfs_movie.iterrows():\n",
    "    if (row['genre'] == 'Documentary') == True:\n",
    "        labelcolor2var.append('red')    #red\n",
    "    elif (row['genre'] == 'Drama,Romance') & (row['year'] == 2007) == True:\n",
    "        labelcolor2var.append('white') #orange\n",
    "    elif (row['genre'] == 'Action,Crime,Drama') == True:\n",
    "        labelcolor2var.append('blue')   #blue\n",
    "    elif (row['genre'] == 'Horror') & (row['year'] == 2007) == True:\n",
    "        labelcolor2var.append('white')  #green\n",
    "    else: \n",
    "        labelcolor2var.append('white')\n",
    "dfs_movie['labelcolor2var']=labelcolor2var    \n",
    "dfs_movie = dfs_movie.loc[dfs_movie['originalTitle'] != 'Geisha vs ninja'] \n",
    "dfs_movie = dfs_movie.loc[dfs_movie['originalTitle'] != 'Dao jiàn xiào'] \n",
    "dfs_2var = dfs_movie[dfs_movie.labelcolor2var != 'white']\n",
    "pd.value_counts(dfs_2var.labelcolor2var).plot.bar() \n",
    "pd.value_counts(dfs_2var.labelcolor2var)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfs_2var' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-804f1975d748>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#tf-idf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#, stop_words=get_stop_words('de'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokens_2var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfs_2var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mhier_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgglomerativeClustering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens_2var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dfs_2var' is not defined"
     ]
    }
   ],
   "source": [
    "#tf-idf\n",
    "tfidf = TfidfVectorizer(max_features=1000) #, stop_words=get_stop_words('de'))\n",
    "tokens_2var = tfidf.fit_transform(dfs_2var.text)\n",
    "hier_model = AgglomerativeClustering(n_clusters=None, distance_threshold=0).fit(tokens_2var.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lmatrix = linkage_matrix(tokens_2var.shape[0], hier_model.children_, hier_model.distances_)\n",
    "plt.figure(figsize=(15,8))\n",
    "dendrogram(lmatrix, labels=(dfs_2var['originalTitle'].values + \"     \" + dfs_2var['genre'].values), leaf_font_size=9);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentary word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentary = \"\"\n",
    "for film in dfs_movie.iterrows():\n",
    "    if film[1].genre == \"Documentary\":\n",
    "        documentary = documentary + film[1].text\n",
    "\n",
    "docu_tokens = re.findall(\"\\w+\", documentary.lower())\n",
    "print(docu_tokens)\n",
    "count_docu = Counter(docu_tokens).most_common(20)\n",
    "count_docu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count by moviename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_bytitle = dfs_movie.loc[dfs_movie['originalTitle'] == 'Contraband']\n",
    "text=\"\"\n",
    "for film in dfs_bytitle.iterrows():\n",
    "    text = text + film[1].text\n",
    "title_tokens = re.findall(\"\\w+\", text.lower())\n",
    "title_docu = Counter(title_tokens).most_common(20)\n",
    "title_docu\n",
    "#Quebrando o Tabu\n",
    "#dfs_bytitle.head()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action, Crime, Drama word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acd = \"\"\n",
    "for film in dfs_movie.iterrows():\n",
    "    if film[1].genre == \"Action,Crime,Drama\":\n",
    "        acd = acd + film[1].text\n",
    "\n",
    "acd_tokens = re.findall(\"\\w+\", acd.lower())\n",
    "count_acd = Counter(acd_tokens).most_common(20)\n",
    "count_acd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = []\n",
    "for film in dfs_movie.iterrows():\n",
    "    tokens = re.findall(\"\\w+\", film[1].text.lower())\n",
    "    count = Counter(tokens).most_common(10)\n",
    "    #print(film[1].primaryTitle + \":  \", count)\n",
    "    wordfreq.append(count)\n",
    "dfs_movie['wordfreq']=wordfreq\n",
    "dfs_movie.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "tokens_2var = tfidf.fit_transform(dfs_2var.text)\n",
    "gmm = mixture.GaussianMixture(n_components=2, covariance_type='diag')\n",
    "gmm_label = gmm.fit_predict(tokens_2var.toarray())\n",
    "gmm.converged_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(tokens_2var.todense())\n",
    "X_tf = pca.transform(tokens_2var.todense())\n",
    "\n",
    "#Abspeichern der Zwischenergebnisste im Datafram\n",
    "dfs_2var['gmm1'] = X_tf[:,0]\n",
    "dfs_2var['gmm2'] = X_tf[:,1]\n",
    "dfs_2var['gmmlabel'] = gmm_label\n",
    "print('Comedy, Crime, Drama in rot VS Comedy, Horror in blau')\n",
    "#Plotten anhand der gespeicherten Zwischenergebnisse \n",
    "plt.figure(figsize=(15,8))\n",
    "for d in dfs_2var.iterrows():\n",
    "    x = d[1].gmm1\n",
    "    y = d[1].gmm2    \n",
    "    plt.plot(x, y, 'o', markersize=14, color= d[1].labelcolor2var)\n",
    "    plt.plot(x, y, marker=TextPath((-3, -3), f'{d[1].gmmlabel}'), color='white', markersize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
